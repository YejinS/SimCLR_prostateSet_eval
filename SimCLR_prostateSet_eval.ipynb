{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YejinS/SimCLR_prostateSet_eval/blob/main/SimCLR_prostateSet_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUemQib7ZE4D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSgRE1CcLqdS"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOIJEui1ZziV"
      },
      "outputs": [],
      "source": [
        "def get_file_id_by_model(folder_name):\n",
        "  file_id = {'resnet18_100-epochs_stl10': '14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF',\n",
        "             'resnet18_100-epochs_cifar10': '1lc2aoVtrAetGn0PnTkOyFzPCIucOJq7C',\n",
        "             'resnet50_50-epochs_stl10': '1ByTKAUsdm_X7tLcii6oAEl5qFRqRMZSu'}\n",
        "  return file_id.get(folder_name, \"Model not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7YMxsvEZMrX",
        "outputId": "af6fce3b-20e9-4174-fac8-f3c1b25db6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet18_100-epochs_stl10 14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF\n"
          ]
        }
      ],
      "source": [
        "folder_name = 'resnet18_100-epochs_stl10'\n",
        "file_id = get_file_id_by_model(folder_name)\n",
        "print(folder_name, file_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWZ8fet_YoJm",
        "outputId": "e85d4ed3-317c-40b7-bc8f-68af14a376b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.yml  drive  runs  sample_data\n"
          ]
        }
      ],
      "source": [
        "# download and extract model files\n",
        "os.system('gdown https://drive.google.com/uc?id={}'.format(file_id))\n",
        "os.system('unzip {}'.format(folder_name))\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_nypQVEv-hn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EckSdD-t7zg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQwsMwRE4SNG",
        "outputId": "22f88f9e-256e-473d-94a0-efa0da375546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDfbL3w_Z0Od",
        "outputId": "499a7ce0-8d18-4e2f-b624-351a947693c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxtDYzUPofv"
      },
      "source": [
        "## Custom Dataset (Prostate Dataset) 정의 & 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU_WxM1kpSsY"
      },
      "outputs": [],
      "source": [
        "#Prostate Dataset 클래스 정의\n",
        "\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "class prostate_dataset(Dataset):\n",
        "    def __init__(self, csv_file, root, download, split, transform=None):\n",
        "        self.train_image = pd.read_csv(csv_file)\n",
        "        self.valid_image = pd.read_csv(csv_file)\n",
        "        self.test_image = pd.read_csv(csv_file)\n",
        "\n",
        "        self.train_label = pd.read_csv(csv_file)\n",
        "        self.valid_label = pd.read_csv(csv_file)\n",
        "        self.test_label = pd.read_csv(csv_file)\n",
        "\n",
        "        self.root = root\n",
        "        self.download = download\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.shape = self.__getshape__()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split==\"train\":\n",
        "            return len(self.train_image)\n",
        "        elif self.split==\"valid\":\n",
        "            return len(self.valid_image)\n",
        "        elif self.split==\"test\": \n",
        "            return len(self.test_image)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if self.split=='train':\n",
        "            img_name = os.path.join(self.root, self.train_image.iloc[idx, 0])\n",
        "            image = io.imread(img_name)\n",
        "            image = torch.tensor(image)\n",
        "            image = torch.unsqueeze(image, 0).repeat(3,1,1)\n",
        "            label = self.train_label.iloc[idx, 1]\n",
        "        elif self.split=='valid':\n",
        "            img_name = os.path.join(self.root, self.valid_image.iloc[idx, 0])\n",
        "            image = io.imread(img_name)\n",
        "            image = torch.tensor(image)\n",
        "            image = torch.unsqueeze(image, 0).repeat(3,1,1)\n",
        "            label = self.valid_label.iloc[idx, 1]\n",
        "        elif self.split=='test':\n",
        "            img_name = os.path.join(self.root, self.test_image.iloc[idx, 0])\n",
        "            image = io.imread(img_name)\n",
        "            image = torch.tensor(image)\n",
        "            image = torch.unsqueeze(image, 0).repeat(3,1,1)\n",
        "            label = self.test_label.iloc[idx, 1]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __getshape__(self):\n",
        "        return (self.__len__(), *self.__getitem__(0)[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfIPl0G6_RrT"
      },
      "outputs": [],
      "source": [
        "#Data Loader 정의\n",
        "def get_stl10_data_loaders(download, shuffle=False, batch_size=256):\n",
        "  train_dataset = datasets.STL10('./data', split='train', download=download,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                            num_workers=0, drop_last=False, shuffle=shuffle)\n",
        "  \n",
        "  test_dataset = datasets.STL10('./data', split='test', download=download,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
        "                            num_workers=10, drop_last=False, shuffle=shuffle)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "def get_cifar10_data_loaders(download, shuffle=False, batch_size=256):\n",
        "  train_dataset = datasets.CIFAR10('./data', train=True, download=download,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                            num_workers=0, drop_last=False, shuffle=shuffle)\n",
        "  \n",
        "  test_dataset = datasets.CIFAR10('./data', train=False, download=download,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size, #2*batch_size 부분 확인\n",
        "                            num_workers=10, drop_last=False, shuffle=shuffle)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "#영상에서 trainset이 여기서는 train_dataset\n",
        "def get_prostate_data_loaders(download, shuffle=False, batch_size=16):\n",
        "  train_dataset = prostate_dataset('./drive/MyDrive/ColabNotebooks/data/prostate_trainSet.csv', './drive/MyDrive/ColabNotebooks/data/datasets/', \n",
        "                                   download=download, split='train', transform=transforms.ToTensor())\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=False, shuffle=shuffle)\n",
        "\n",
        "  valid_dataset = prostate_dataset('./drive/MyDrive/ColabNotebooks/data/prostate_valSet.csv', './drive/MyDrive/ColabNotebooks/data/datasets/', \n",
        "                                   download=download, split='valid', transform=transforms.ToTensor())\n",
        "\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=0, drop_last=False, shuffle=shuffle)\n",
        "  \n",
        "  test_dataset = prostate_dataset('./drive/MyDrive/ColabNotebooks/data/prostate_testSet.csv', './drive/MyDrive/ColabNotebooks/data/datasets/', \n",
        "                                  download=download, split='test', transform=transforms.ToTensor())\n",
        "\n",
        "  test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, drop_last=False, shuffle=shuffle)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N8lYkbmDTaK"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('./config.yml')) as file:\n",
        "  config = yaml.load(file, Loader=yaml.Loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a18lPD-tIle6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e67e1b8-8fdf-43d1-b9e1-15efb5f4fcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "if config.arch == 'resnet18':\n",
        "  model = torchvision.models.resnet18(pretrained=False, num_classes=2).to(device)\n",
        "elif config.arch == 'resnet50':\n",
        "  model = torchvision.models.resnet50(pretrained=False, num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AIfgq41GuTT"
      },
      "outputs": [],
      "source": [
        "#checkpoint = torch.load('./drive/MyDrive/ColabNotebooks/checkpoint_0400.pth.tar', map_location=device)\n",
        "checkpoint = torch.load('./drive/MyDrive/ColabNotebooks/epoch500ES329lr00001wd0004batch4.pth.tar', map_location=device)\n",
        "#checkpoint = torch.load('./drive/MyDrive/ColabNotebooks/checkpoint_0750_stl10+promiseX.pth.tar', map_location=device)\n",
        "#checkpoint = torch.load('./drive/MyDrive/ColabNotebooks/epochSTL400PROMISE500batch256+4+2acc100loss3dotxx.pth.tar', map_location=device)\n",
        "#checkpoint = torch.load('./drive/MyDrive/ColabNotebooks/epoch400+500tune200to100batch256+4+2lr0003+001.pth.tar', map_location=device)\n",
        "\n",
        "state_dict = checkpoint['state_dict']\n",
        "\n",
        "for k in list(state_dict.keys()): \n",
        "  #print(k)\n",
        "  if k.startswith('backbone.'):\n",
        "    if k.startswith('backbone') and not k.startswith('backbone.fc'):\n",
        "      # remove prefix ex. backbone.layer -> layer\n",
        "      state_dict[k[len(\"backbone.\"):]] = state_dict[k] #backbone.layer3.0.bn2.bias 이런식으로 되어있던 key에다가 parameter 텐서들 싹 다 backbone. 없앤 layer3.0.bn2.bias 형태의 새로운 키 이름에다가 복사해주고 원래 있언 backbone.layer3.0.bn2.bias 들의 키들은 싹 다 지워주기\n",
        "  del state_dict[k] #이 셀을 거치고 나면 각 키들 앞에 \"backbone\" 이란 글씨랑 fc layer 부분이 사라짐"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVjA83PPJYWl"
      },
      "outputs": [],
      "source": [
        "log = model.load_state_dict(state_dict, strict=False) #위에 새롭게 생성한 키들(backbone. 없는 새로운 이름들, fc 레이어도 없음)\n",
        "assert log.missing_keys == ['fc.weight', 'fc.bias'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GC0a14uWRr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07b3ba1-0d87-48a4-a7a8-cebd8b470d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: prostate\n"
          ]
        }
      ],
      "source": [
        "if config.dataset_name == 'cifar10':\n",
        "  train_loader, test_loader = get_cifar10_data_loaders(download=True)\n",
        "elif config.dataset_name == 'stl10':\n",
        "  train_loader, test_loader = get_stl10_data_loaders(download=True)\n",
        "elif config.dataset_name == 'prostate':\n",
        "  train_loader, valid_loader, test_loader = get_prostate_data_loaders(download=False)\n",
        "print(\"Dataset:\", config.dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLkl-r5wQYso"
      },
      "source": [
        "## 네트워크 & 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYT_KsM0Mnnr"
      },
      "outputs": [],
      "source": [
        "# # freeze all layers but the last fc\n",
        "# for name, param in model.named_parameters():\n",
        "#     #print(name)\n",
        "#     if name not in ['fc.weight', 'fc.bias']:\n",
        "#         param.requires_grad = False \n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "      param.requires_grad = True #requires_grad = True 로 설정하면 텐서에 대한 기울기를 저장하게 됩니다.\n",
        "      #print(name)\n",
        "\n",
        "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "assert len(parameters) == 62  #requires_grad = True 된 layer 갯수 assert 문으로 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPVh1S_eMRDU"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLv30dljQg7d"
      },
      "source": [
        "## 정확도 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edr6RhP2PdVq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):  #top1 의 \"값\"을 뽑아야함!! confidence\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0) \n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True) #torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor)\n",
        "        accuracy._, accuracy.prediction = output.topk(maxk, 1, True, True)\n",
        "        #accuracy.softmax = nn.functional.softmax(_)\n",
        "        pred = pred.t() #t메소트 : ex) [[1,2,3],[4,5,6]] => [[1,4],[2,5],[3,6]]\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mDvlzo-Qk2P"
      },
      "source": [
        "## train & valid & test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOder0dAMI7X"
      },
      "outputs": [],
      "source": [
        "epochs = 60\n",
        "for epoch in range(epochs):\n",
        "  top1_train_accuracy = 0\n",
        "  train_loss =0\n",
        "  valid_loss =0\n",
        "  model.train()\n",
        "  for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.float()\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    logits = model(x_batch)\n",
        "    loss = criterion(logits, y_batch)\n",
        "    train_loss +=loss.item()\n",
        "    top1 = accuracy(logits, y_batch, topk=(1,))\n",
        "    # writer.add_scalar(\"loss/train\", loss, epoch)\n",
        "    # writer.add_scalar(\"top1/train\", top1[0], epoch)\n",
        "    top1_train_accuracy += top1[0]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  top1_train_accuracy /= (counter + 1)\n",
        "  top1_valid_accuracy = 0\n",
        "\n",
        "  # evaluate model:\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for counter, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "      x_batch = x_batch.float()\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      logits = model(x_batch)\n",
        "      loss = criterion(logits, y_batch)\n",
        "      valid_loss +=loss.item()\n",
        "      top1 = accuracy(logits, y_batch, topk=(1,))\n",
        "      # writer.add_scalar(\"loss/valid\", loss, epoch)\n",
        "      # writer.add_scalar(\"top1/valid\", top1[0], epoch)\n",
        "      top1_valid_accuracy += top1[0]\n",
        "  \n",
        "    top1_valid_accuracy /= (counter + 1)\n",
        "\n",
        "  train_loss = train_loss/(len(train_loader))\n",
        "  valid_loss = valid_loss/(len(valid_loader))\n",
        "\n",
        "  # writer.add_scalar(\"top1\", {'train_acc':top1_train_accuracy.item(), 'val_acc':top1_valid_accuracy.item()}, epoch)\n",
        "  # writer.add_scalar(\"loss\", {'train_loss':train_loss, 'val_loss':valid_loss}, epoch)\n",
        "  writer.add_scalar(\"top1/train\", top1_train_accuracy.item(), epoch)\n",
        "  writer.add_scalar(\"top1/valid\", top1_valid_accuracy.item(), epoch)\n",
        "  writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
        "  writer.add_scalar(\"loss/valid\", valid_loss, epoch)\n",
        "  print(f\"Epoch {epoch}\\tTop1 Train accuracy {top1_train_accuracy.item()}\\tTop1 Valid accuracy: {top1_valid_accuracy.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "probability=[]\n",
        "\n",
        "top1_test_accuracy = 0\n",
        "for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
        "  x_batch = x_batch.float()\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "  \n",
        "  y_true.extend(y_batch.cpu().numpy()) #the .numpy() method is pretty much straightforward. It converts a tensor object into an numpy.ndarray object\n",
        "\n",
        "  logits = model(x_batch)\n",
        "  \n",
        "  test_top1 = accuracy(logits, y_batch, topk=(1,))\n",
        "  y_pred.extend(accuracy.prediction.cpu().numpy().tolist())\n",
        "  y_pred2 = np.array(y_pred).flatten().tolist()\n",
        "  softmax = nn.functional.softmax(logits, dim=1) #softmax 적용 부분\n",
        "  probability.extend(softmax.cpu().detach().numpy())\n",
        "  probability2 = np.array(probability).flatten()\n",
        "\n",
        "  top1_test_accuracy += test_top1[0]\n",
        "\n",
        "top1_test_accuracy /= (counter + 1)\n",
        "\n",
        "print(f\"Total \\tTop1 Test accuracy: {top1_test_accuracy.item()}\") "
      ],
      "metadata": {
        "id": "0ymh6mNav5Vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5647d65a-4090-49ba-9915-9376be59c0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total \tTop1 Test accuracy: 51.61289978027344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_true)\n",
        "print(y_pred2)\n",
        "print(probability2.round(2))"
      ],
      "metadata": {
        "id": "KqQnJPe34zAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred2)"
      ],
      "metadata": {
        "id": "IrsgFtAJlBvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ('0','1') #label0==g1 label1==g2\n",
        "\n",
        "# Create pandas dataframe\n",
        "dataframe = pd.DataFrame(cf_matrix, index=class_names, columns=class_names)\n",
        "dataframe"
      ],
      "metadata": {
        "id": "IKQU6PiQlWO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# Create heatmap\n",
        "matrixGraph = sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"YlGnBu\",fmt=\"d\")\n",
        "matrixGraph.xaxis.tick_top()\n",
        "matrixGraph.xaxis.set_label_position('top')\n",
        "matrixGraph.invert_yaxis()\n",
        "matrixGraph.invert_xaxis()\n",
        "\n",
        "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
        "plt.ylabel(\"True Class\"), \n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "J1Mpb9GL24hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = cf_matrix[1,1] # true positive\n",
        "TN = cf_matrix[0,0] # true negatives\n",
        "FP = cf_matrix[1,0] # false positives\n",
        "FN = cf_matrix[0,1] # false negatives\n",
        "\n",
        "P=TP+FN\n",
        "N=TN+FP\n",
        "\n",
        "TP, TN, FP, FN"
      ],
      "metadata": {
        "id": "iU_qd63m4t9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitivity = TP/(TP+FN)\n",
        "specificity = TN/(FP+TN)\n",
        "ppv = TP/(TP+FP)\n",
        "npv = TN/(TN+FN)"
      ],
      "metadata": {
        "id": "7yFwjua3_-Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sensitivity: \",sensitivity)\n",
        "print(\"specificity: \",specificity)\n",
        "print(\"ppv: \",ppv)\n",
        "print(\"npv: \",npv)"
      ],
      "metadata": {
        "id": "Khsmiwt3AY1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmW673scQafy"
      },
      "outputs": [],
      "source": [
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYz_4cYNQqGs"
      },
      "source": [
        "## 학습 결과 나타내기(Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Leaq4h-YT70"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pwd"
      ],
      "metadata": {
        "id": "lwvqoYOZmDb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r ./runs"
      ],
      "metadata": {
        "id": "s1dwhn5nwAti"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}